# Letter-Frequency-Analysis
This program performs an analysis of a set of books to determine how well the distribution of letters in these texts corresponds to that found by other researchers.

In this program I will be performing an analysis of a set of books to determine how well the distribution of letters in these texts corresponds to that found by other researchers. A chart of letter frequencies in the English language can be found at https://en.wikipedia.org/wiki/Letter_frequency .

This program should uses command line arguments to input the names of several books that are being analyzed. There are ten such books that I got from Project Gutenberg given above. These are all long texts, so that the sample size for the analysis will be reasonable. For each book, the program will read in the text and count how many times each letter occurs in the text. The program ignores all numbers and symbols that you find in the text, and upper and lower case letters are considered the same for the counting process. The program then computes the occurrence of percentages for each letter in each book to generate a frequency table like that mentioned above. The program uses a dictionary to store this information.

Once we have a letter frequency table for each book, we can generate some reports. First, I print out a table showing the difference in frequency values from that observed in each book to that in the wikipedia table. Then we sum up the absolute value of each letter's error to get a total error, which is also printed for each book. Then we rank the books by this total error amount, from that which is closest to the given table, to that which is the most different from the table. Then we determine whether the nationality of the book author has any effect on the difference in letter frequency. From the books I have taken, there are American, Russian, French, Spanish, English and Irish authors.

Secondly, we will sort each of the tables from most frequent to least frequent and then compare each book's sorted listing to that of a sorted version of the wikipedia table. Also determine whether each letter is in the same position, or if not, how many positions difference there is up or down in the ranking. Then print out another table containing these changes in ranking as positive and negative values. Next, Sum up the absolute value of each letter's change in position to get a total error, which is also printed for each book. Then rank the books by this total change amount, from that which is closest to the given table, to that which is the most different from the table. 

For the final analysis, I begin with the sorted versions of the letter frequencies. Then group these orderings in four groups: positions 1-6, 7-12, 13-19 and 20-26. For each sorted list, the program computes the set of letters for each of these groupings for each book. Then, using set intersection, determines which letters are present within these four groups across all of the books. Prints this information out. Then, using set union, analyzes two groupings: 1-12 and 13-26, to see which letters are always within these two groupings across all books. Prints this information out.
